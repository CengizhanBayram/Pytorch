{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2468b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# %% library\n",
    "\n",
    "import torch                               # PyTorch kütüphanesi, tensor işlemleri\n",
    "import torch.nn as nn                      # Yapay sinir ağı katmanlarını tanımlamak için\n",
    "import torch.optim as optim                # Optimizasyon algoritmalarını içeren modül\n",
    "import torchvision                         # Görüntü işleme ve pre-defined modelleri içerir\n",
    "import torchvision.transforms as transforms  # Görüntü dönüşümleri yapmak için\n",
    "import matplotlib.pyplot as plt            # Görselleştirme\n",
    "\n",
    "# optional: cihazı belirle (GPU vs CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9af1bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size=64):\n",
    "    # Görüntüleri tensöre çevir ve normalize et:\n",
    "    # 1. ToTensor(): Görüntüyü [0, 255] aralığından [0.0, 1.0] aralığına çevirir ve (H x W x C) → (C x H x W) yapar\n",
    "    # 2. Normalize(mean=[0.5], std=[0.5]): Her pikseli (x - 0.5) / 0.5 ile normalize eder → [0,1] → [-1,1] aralığına çeker\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  \n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])#compose belirli dönüşüm zinciir oluşturmak için kullanılır\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.MNIST(root=\"./data\", train=True,download=True ,transform=transform)\n",
    "    test_set = torchvision.datasets.MNIST(root=\"./data\", train= False,download=True ,transform=transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "    return train_loader,test_loader\n",
    "train_loader,test_loader= get_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc862e",
   "metadata": {},
   "source": [
    "data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "174bc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")      # GUI gerekmez, çizimler hücrede gösterilir\n",
    "import matplotlib.pyplot as plt\n",
    "      # ③ pyplot artık doğru backend ile\n",
    "\n",
    "\n",
    "# data visualization\n",
    "def visualize_samples(loader, n):\n",
    "    # İlk batch'ten görüntüleri ve etiketleri alalım\n",
    "    images, labels = next(iter(loader))\n",
    "\n",
    "    # n adet görseli yatay şekilde göstermek için subplot oluştur\n",
    "    fig, axes = plt.subplots(1, n, figsize=(10, 5))  # n farklı görüntü için görselleştirme alanı\n",
    "\n",
    "    for i in range(n):\n",
    "        # Görseli sıkıştır (tek kanallı hale getir), gri tonlamalı olarak göster\n",
    "        axes[i].imshow(images[i].squeeze(), cmap=\"gray\")\n",
    "        \n",
    "        # Başlığa etiket yaz\n",
    "        axes[i].set_title(f\"Label: {labels[i].item()}\")\n",
    "        \n",
    "        # Eksenleri gizle\n",
    "        axes[i].axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "visualize_samples(test_loader , 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adda1fb5",
   "metadata": {},
   "source": [
    "yapay sinir ağı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0bec5255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'NeuralNetwork' adında bir sınıf tanımlıyoruz. Bu sınıf, PyTorch'un temel sinir ağı modülü olan 'nn.Module' sınıfından miras alır.\n",
    "# Bu, PyTorch'un bu sınıfı bir sinir ağı modeli olarak tanımasını sağlar.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    # Bu, sınıfın yapıcı (constructor) metodudur. Sinir ağının katmanları burada tanımlanır.\n",
    "    def __init__(self):\n",
    "        # 'super()' fonksiyonu, 'nn.Module' ana sınıfının yapıcı metodunu çağırır. Bu, modelin doğru bir şekilde kurulması için gereklidir.\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # 'nn.Flatten()' katmanını oluşturuyoruz. Bu katman, çok boyutlu girdileri (örneğin 28x28 piksellik bir resim) tek boyutlu bir vektöre dönüştürür.\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # İlk tam bağlantılı (fully connected) katmanı ('Linear' katman) tanımlıyoruz.\n",
    "        # Bu katman 28*28 = 784 adet girdiyi alır ve bunları 128 adet çıktıya dönüştürür.\n",
    "        # Genellikle MNIST gibi 28x28 piksellik resim veri setleri için kullanılır.\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        \n",
    "        # ReLU (Rectified Linear Unit) aktivasyon fonksiyonunu tanımlıyoruz.\n",
    "        # Bu fonksiyon, modele non-linear (doğrusal olmayan) bir özellik kazandırır. Negatif değerleri sıfır yapar, pozitif değerleri ise olduğu gibi bırakır.\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # İkinci tam bağlantılı katmanı tanımlıyoruz.\n",
    "        # Bir önceki katmanın çıktısı olan 128 özelliği alır ve bunları 64 özelliğe indirger.\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        \n",
    "        # !!! DİKKAT: HATA POTANSİYELİ !!!\n",
    "        # Üçüncü tam bağlantılı katmanı tanımlıyoruz. Bu katman 54 girdi alacak şekilde tanımlanmış.\n",
    "        # Ancak bir önceki katman olan 'self.fc2', 64 adet çıktı üretir.\n",
    "        # Girdi (54) ve çıktı (64) boyutları uyuşmadığı için bu kod 'forward' metodunda hata verecektir.\n",
    "        # Doğrusu: self.fc3 = nn.Linear(64, 10) olmalıydı.\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "\n",
    "    # 'forward' metodu, verinin (x) ağ içinde nasıl akacağını tanımlar (ileri yayılım - forward propagation).\n",
    "    def forward(self, x):\n",
    "        # Girdi verisi 'x'i 'flatten' katmanından geçirerek düzleştiriyoruz.\n",
    "        # Örneğin [batch_size, 1, 28, 28] boyutundaki bir resim tensörü [batch_size, 784] boyutunda bir vektöre dönüşür.\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Düzleştirilmiş veriyi ilk tam bağlantılı katman olan 'fc1'den geçiriyoruz.\n",
    "        x = self.fc1(x) \n",
    "        \n",
    "        # 'fc1' katmanının çıktısını 'relu' aktivasyon fonksiyonundan geçiriyoruz.\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Aktivasyon fonksiyonundan geçen veriyi ikinci tam bağlantılı katman olan 'fc2'den geçiriyoruz.\n",
    "        # Bu noktada 'x'in boyutu [batch_size, 64] olur.\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # 'fc2'den çıkan 64 boyutlu tensör, 54 girdi bekleyen 'fc3'e gönderilmeye çalışılacağı için burada bir boyut uyuşmazlığı hatası ('runtime error') alınacaktır.\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        # Bu satır gereksizdir ve muhtemelen bir hatadır.\n",
    "        # 'fc3' katmanının çıktısı zaten [batch_size, 10] boyutunda düz bir tensördür.\n",
    "        # Zaten düz olan bir veriyi tekrar düzleştirmeye çalışmak anlamsızdır ve genellikle bir etkisi olmaz.\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Metodun sonunda, ağın son çıktısı olan 'x' döndürülür (hata olmasaydı).\n",
    "        return x\n",
    "# Gerekli kütüphaneleri içe aktardığımızı varsayalım.\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# 'define_loss_and_optimizer' adında bir lambda (anonim) fonksiyon tanımlıyoruz.\n",
    "# Bu fonksiyon, parametre olarak bir 'model' (sinir ağı modeli) alır.\n",
    "define_loss_and_optimizer = lambda model: (\n",
    "    \n",
    "    # Fonksiyonun döndüreceği ilk eleman: Kayıp Fonksiyonu (Loss Function).\n",
    "    # 'nn.CrossEntropyLoss()', çok sınıflı sınıflandırma problemleri için yaygın olarak kullanılan bir kayıp fonksiyonudur.\n",
    "    # Bu fonksiyon, kendi içinde bir 'LogSoftmax' ve 'NLLLoss' (Negative Log Likelihood Loss) katmanlarını birleştirir.\n",
    "    # Bu nedenle, modelin son katmanında ayrıca bir Softmax fonksiyonu kullanmaya gerek yoktur.\n",
    "    # Modelin tahminlerinin (logits) gerçek etiketlerden ne kadar uzak olduğunu ölçer.\n",
    "    nn.CrossEntropyLoss(),  # multi class classification problems loss function\n",
    "\n",
    "    # Fonksiyonun döndüreceği ikinci eleman: Optimizasyon Algoritması (Optimizer).\n",
    "    # 'optim.Adam', Stokastik Gradyan İnişi'nin (SGD) bir uzantısı olan ve uyarlanabilir öğrenme oranları kullanan popüler bir optimizasyon algoritmasıdır.\n",
    "    # Parametreler:\n",
    "    # 1. model.parameters(): Optimize edilecek olan parametreleri (ağın öğrenilebilir ağırlıkları ve bias'ları) belirtir.\n",
    "    # 2. lr = 0.001: Öğrenme oranını (learning rate) belirtir. Bu, her adımda modelin ağırlıklarının ne kadar güncelleneceğini kontrol eden bir hiperparametredir.\n",
    "    optim.Adam(model.parameters(), lr = 0.001)\n",
    ")\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b465fc",
   "metadata": {},
   "source": [
    "traiin model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3063642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri içe aktardığımızı varsayalım.\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# %% train başlığı sadece bir hücre ayıracıdır, genellikle Jupyter Notebook veya benzeri ortamlarda kullanılır.\n",
    "\n",
    "# Modelin eğitimini gerçekleştirecek olan fonksiyonu tanımlıyoruz.\n",
    "# Parametreler:\n",
    "# model: Eğitilecek PyTorch modeli.\n",
    "# train_loader: Eğitim verilerini içeren DataLoader nesnesi. Verileri mini-batch'ler halinde sunar.\n",
    "# criterion: Kayıp fonksiyonu (örneğin nn.CrossEntropyLoss).\n",
    "# optimizer: Optimizasyon algoritması (örneğin optim.Adam).\n",
    "# epochs: Veri setinin tamamı üzerinden kaç kez geçileceği (eğitim turu sayısı). Varsayılan değeri 10'dur.\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs = 10):\n",
    "\n",
    "    # model.train() komutu, modeli \"eğitim moduna\" alır.\n",
    "    # Bu mod, Dropout ve BatchNorm gibi katmanların eğitim sırasında aktif olmasını sağlar.\n",
    "    # (Değerlendirme için model.eval() kullanılır.)\n",
    "    model.train() \n",
    "    \n",
    "    # Her bir epoch (eğitim turu) sonunda hesaplanan ortalama kayıp değerlerini saklamak için boş bir liste oluşturuyoruz.\n",
    "    train_losses = [] \n",
    "    \n",
    "    # Belirtilen epoch sayısı kadar döngü başlatıyoruz. Bu, ana eğitim döngüsüdür.\n",
    "    for epoch in range(epochs): \n",
    "        # Her epoch'un başında o epoch için toplam kaybı sıfırlıyoruz.\n",
    "        total_loss = 0 \n",
    "        \n",
    "        # 'train_loader' içindeki tüm veri yığınları (batch'ler) üzerinde iterasyon yapıyoruz.\n",
    "        # Her adımda, loader bize bir yığın resim ('images') ve bunlara karşılık gelen etiketleri ('labels') verir.\n",
    "        for images, labels in train_loader: \n",
    "            \n",
    "            # Veri ve etiketleri, hesaplamanın yapılacağı cihaza (CPU veya GPU) taşıyoruz.\n",
    "            # 'device' değişkeninin daha önce tanımlanmış olması gerekir (örn: device = \"cuda\" if torch.cuda.is_available() else \"cpu\").\n",
    "            images, labels = images.to(device), labels.to(device) \n",
    "            \n",
    "            # Gradyanları sıfırlıyoruz. PyTorch gradyanları her geri yayılımda biriktirir.\n",
    "            # Bu yüzden her yeni yığın için hesaplama yapmadan önce eski gradyanları temizlememiz gerekir.\n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            # Modeli kullanarak girdilerden (images) tahminler (predictions) üretiyoruz.\n",
    "            # Bu işlem, verinin ağ içinden ileri doğru akışını (forward propagation) temsil eder.\n",
    "            predictions = model(images) \n",
    "            \n",
    "            # Kayıp fonksiyonunu (criterion) kullanarak modelin tahminleri (predictions) ile gerçek etiketler (labels) arasındaki hatayı (kaybı) hesaplıyoruz.\n",
    "            loss = criterion(predictions, labels) \n",
    "            \n",
    "            # 'loss.backward()' metodu, hesaplanan kayba göre modelin tüm öğrenilebilir parametreleri için gradyanları hesaplar (geri yayılım - backpropagation).\n",
    "            loss.backward() \n",
    "            \n",
    "            # 'optimizer.step()' metodu, hesaplanan gradyanları kullanarak modelin ağırlıklarını günceller.\n",
    "            optimizer.step() \n",
    "            \n",
    "            # O anki yığının kaybını (.item() ile tensörden Python sayısına çevirerek) toplam kayba ekliyoruz.\n",
    "            total_loss = total_loss + loss.item() \n",
    "        \n",
    "        # Epoch'taki tüm yığınlar işlendikten sonra, ortalama kaybı hesaplıyoruz.\n",
    "        # Toplam kayıp, veri yükleyicideki yığın sayısına bölünür.\n",
    "        avg_loss = total_loss / len(train_loader) \n",
    "        \n",
    "        # Hesaplanan ortalama kaybı, daha sonra grafiğini çizmek üzere listemize ekliyoruz.\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Mevcut epoch numarasını ve o epoch'a ait ortalama kaybı ekrana yazdırıyoruz.\n",
    "        # f-string formatı, {avg_loss:.3f} ile kaybı ondalık noktadan sonra 3 basamakla gösterir.\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.3f}\")\n",
    "\n",
    "    # Eğitim döngüsü bittikten sonra kayıp grafiğini çizmek için bölüm.\n",
    "    # Yeni bir çizim figürü oluştur.\n",
    "    plt.figure() \n",
    "    \n",
    "    # Kayıp grafiğini çiz.\n",
    "    # x ekseni: epoch sayıları (1'den başlar), y ekseni: her epoch'taki kayıp değerleri.\n",
    "    # marker='o': her veri noktasını daire ile işaretle.\n",
    "    # linestyle='-': noktaları düz bir çizgiyle birleştir.\n",
    "    # label='Train Loss': grafiğin etiketini belirle.\n",
    "    plt.plot(range(1, epochs + 1), train_losses, marker = \"o\", linestyle = \"-\", label = \"Train Loss\")\n",
    "    \n",
    "    # x ekseninin etiketini ayarla.\n",
    "    plt.xlabel(\"Epochs\") \n",
    "    \n",
    "    # y ekseninin etiketini ayarla.\n",
    "    plt.ylabel(\"Loss\") \n",
    "    \n",
    "    # Grafiğin başlığını ayarla.\n",
    "    plt.title(\"Training Loss\")\n",
    "    \n",
    "    # Grafikteki etiketleri ('Train Loss') göstermek için bir lejant ekle.\n",
    "    plt.legend()\n",
    "    \n",
    "    # Hazırlanan grafiği ekranda göster.\n",
    "    plt.show()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3ac7389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.368\n",
      "Epoch 2/5, Loss: 0.170\n",
      "Epoch 3/5, Loss: 0.129\n",
      "Epoch 4/5, Loss: 0.109\n",
      "Epoch 5/5, Loss: 0.093\n"
     ]
    }
   ],
   "source": [
    "# %% test başlığı, genellikle bir sonraki kod hücresinin \"test\" ile ilgili olduğunu belirtmek için kullanılır.\n",
    "\n",
    "# Daha önce tanımladığımız 'train_model' fonksiyonunu burada çağırıyoruz.\n",
    "# Bu satır, asıl eğitim sürecini başlatan komuttur.\n",
    "# Fonksiyona aşağıdaki argümanları iletiyoruz:\n",
    "# 1. model: Eğitmek istediğimiz, daha önceden oluşturulmuş sinir ağı modeli.\n",
    "# 2. train_loader: Eğitim verilerini yığınlar (batch) halinde içeren DataLoader nesnesi.\n",
    "# 3. criterion: Modelin hatasını ölçmek için tanımlanmış kayıp fonksiyonu (örneğin CrossEntropyLoss).\n",
    "# 4. optimizer: Modelin ağırlıklarını güncellemek için tanımlanmış optimizasyon algoritması (örneğin Adam).\n",
    "# 5. epochs=5: Eğitim sürecinin toplam 5 tur (epoch) süreceğini belirtiyoruz. \n",
    "#    Veri setinin tamamı üzerinden 5 kez geçilecektir.\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fe9f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.178%\n"
     ]
    }
   ],
   "source": [
    "# Gerekli kütüphaneleri içe aktardığımızı varsayalım.\n",
    "# import torch\n",
    "\n",
    "# %% test başlığı, bu hücrenin test işlemleriyle ilgili olduğunu belirtir.\n",
    "\n",
    "# Modelin test performansını ölçecek olan fonksiyonu tanımlıyoruz.\n",
    "# Parametreler:\n",
    "# model: Değerlendirilecek, eğitilmiş PyTorch modeli.\n",
    "# test_loader: Test verilerini içeren DataLoader nesnesi.\n",
    "def test_model(model, test_loader):\n",
    "\n",
    "    # model.eval() komutu, modeli \"değerlendirme (evaluation) moduna\" alır.\n",
    "    # Bu mod, eğitimde kullanılan Dropout gibi katmanları devre dışı bırakır.\n",
    "    # Bu, test sırasında tutarlı sonuçlar almak için çok önemlidir.\n",
    "    model.eval()\n",
    "\n",
    "    # Doğru tahmin edilen veri sayısını tutmak için bir sayaç başlatıyoruz.\n",
    "    correct = 0\n",
    "    # Toplam veri sayısını tutmak için bir sayaç başlatıyoruz.\n",
    "    total = 0\n",
    "\n",
    "    # 'with torch.no_grad():' bloğu, bu blok içindeki tüm işlemler için gradyan hesaplamasını devre dışı bırakır.\n",
    "    # Test/değerlendirme sırasında ağırlıkları güncellemeyeceğimiz için gradyanlara ihtiyacımız yoktur.\n",
    "    # Bu, gereksiz hesaplamaları önleyerek süreci hızlandırır ve bellek kullanımını azaltır.\n",
    "    with torch.no_grad():\n",
    "        # 'test_loader' içindeki tüm veri yığınları (batch'ler) üzerinde iterasyon yapıyoruz.\n",
    "        for images, labels in test_loader:\n",
    "            # Test verilerini ve etiketlerini, hesaplamanın yapılacağı cihaza (CPU veya GPU) taşıyoruz.\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Modeli kullanarak girdilerden (images) tahminler (predictions) üretiyoruz (ileri yayılım).\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Modelin çıktısı olan olasılık/skorlardan en yüksek olanın indeksini alıyoruz.\n",
    "            # torch.max(predictions, 1) fonksiyonu, her bir örnek için en yüksek değere sahip sınıfın skorunu ve indeksini döndürür.\n",
    "            # 'predictions' tensörünün 1. boyutu (sınıf skorları) üzerinde çalışır.\n",
    "            # Bize sadece indeks (tahmin edilen sınıf) gerektiği için skorları '_' ile görmezden geliyoruz.\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "\n",
    "            # Toplam veri sayacını, mevcut yığındaki (batch) etiket sayısı kadar artırıyoruz.\n",
    "            # labels.size(0) o anki yığının boyutunu (batch size) verir.\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Doğru tahmin sayacını güncelliyoruz.\n",
    "            # (predicted == labels) işlemi, tahminler ile gerçek etiketleri karşılaştırarak bir boolean tensör ([True, False, True...]) üretir.\n",
    "            # .sum() bu tensördeki 'True' (1) değerlerini toplayarak o yığındaki doğru tahmin sayısını bulur.\n",
    "            # .item() ise bu sonucu tek bir sayı olarak Python'a aktarır.\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Tüm test verileri işlendikten sonra, modelin doğruluk oranını (accuracy) hesaplayıp yazdırıyoruz.\n",
    "    # Doğruluk = (Doğru Tahmin Sayısı / Toplam Veri Sayısı) * 100\n",
    "    # {değer:.3f} formatı, sonucu ondalık noktadan sonra 3 basamakla gösterir.\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.3f}%\")\n",
    "\n",
    "# Eğitilmiş 'model' ve 'test_loader' ile test fonksiyonunu çağırıyoruz.\n",
    "# Bu satırın çalışması için 'model'in eğitilmiş ve 'test_loader'ın tanımlanmış olması gerekir.\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd2ba973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.356\n",
      "Epoch 2/10, Loss: 0.174\n",
      "Epoch 3/10, Loss: 0.131\n",
      "Epoch 4/10, Loss: 0.109\n",
      "Epoch 5/10, Loss: 0.096\n",
      "Epoch 6/10, Loss: 0.083\n",
      "Epoch 7/10, Loss: 0.077\n",
      "Epoch 8/10, Loss: 0.071\n",
      "Epoch 9/10, Loss: 0.064\n",
      "Epoch 10/10, Loss: 0.061\n",
      "Test Accuracy: 98.235%\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\": bloğu, Python'da standart bir yapıdır.\n",
    "# Bu blok içindeki kodlar, sadece bu script dosyası doğrudan çalıştırıldığında yürütülür.\n",
    "# Eğer bu script başka bir dosya tarafından 'import' edilirse, bu blok içindeki kodlar çalışmaz.\n",
    "# Bu, kodun modüler olmasını ve ana çalışma mantığının sadece istendiğinde tetiklenmesini sağlar.\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 'get_data_loaders()' adında bir fonksiyonu çağırıyoruz (bu fonksiyonun daha önce tanımlandığını varsayıyoruz).\n",
    "    # Bu fonksiyonun görevi, eğitim ve test veri setlerini yükleyip hazırlamak ve bunları\n",
    "    # PyTorch'un DataLoader nesnelerine dönüştürmektir.\n",
    "    # Sonuç olarak, eğitim için 'train_loader' ve test için 'test_loader' değişkenlerini elde ederiz.\n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "\n",
    "    # 'visualize_samples()' adında bir fonksiyonu çağırıyoruz (bu da önceden tanımlanmış olmalı).\n",
    "    # Bu fonksiyon, muhtemelen 'train_loader'dan 5 adet veri örneği alıp bunları ekranda görselleştirir.\n",
    "    # Bu, verinin doğru yüklenip yüklenmediğini kontrol etmek için yapılan yaygın bir işlemdir.\n",
    "    visualize_samples(train_loader, 5)\n",
    "\n",
    "    # Daha önce tanımladığımız 'NeuralNetwork' sınıfından bir nesne (yani modelimizi) oluşturuyoruz.\n",
    "    # .to(device) metodu ile modeli, hesaplamaların yapılacağı cihaza (CPU veya GPU) taşıyoruz.\n",
    "    model = NeuralNetwork().to(device)\n",
    "\n",
    "    # Kayıp fonksiyonunu (criterion) ve optimizasyon algoritmasını (optimizer) tanımlamak için\n",
    "    # daha önce oluşturduğumuz 'define_loss_and_optimizer' fonksiyonunu çağırıyoruz.\n",
    "    criterion, optimizer = define_loss_and_optimizer(model)\n",
    "\n",
    "    # Modelin eğitim sürecini başlatmak için 'train_model' fonksiyonunu çağırıyoruz.\n",
    "    # Bu fonksiyona gerekli olan tüm bileşenleri (model, veri, kayıp fonksiyonu, optimizer) veriyoruz.\n",
    "    # Epoch sayısı belirtilmediği için fonksiyonun varsayılan değeri (örn: 10) kullanılır.\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "    # 'train_model' fonksiyonu çalışıp modelin eğitimi bittikten sonra bu satıra geçilir.\n",
    "    # 'test_model' fonksiyonunu çağırarak, eğitilmiş modelin performansını daha önce hiç görmediği\n",
    "    # test verileri üzerinde ölçüyoruz. Sonuç olarak test doğruluğu ekrana yazdırılır.\n",
    "    test_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
